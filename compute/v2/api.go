// Code generated from OpenAPI specs by Databricks SDK Generator. DO NOT EDIT.

// These APIs allow you to manage Cluster Policies, Clusters, Command Execution, Global Init Scripts, Instance Pools, Instance Profiles, Libraries, Policy Compliance For Clusters, Policy Families, etc.
package compute

import (
	"context"
	"fmt"
	"time"

	"github.com/databricks/databricks-sdk-go/databricks/retries"
	"github.com/databricks/databricks-sdk-go/databricks/useragent"
)

type clusterPoliciesBaseClient struct {
	clusterPoliciesImpl
}

// Delete a cluster policy.
//
// Delete a policy for a cluster. Clusters governed by this policy can still
// run, but cannot be edited.
func (a *clusterPoliciesBaseClient) DeleteByPolicyId(ctx context.Context, policyId string) (*DeletePolicyResponse, error) {
	return a.clusterPoliciesImpl.Delete(ctx, DeletePolicy{
		PolicyId: policyId,
	})
}

// Get a cluster policy.
//
// Get a cluster policy entity. Creation and editing is available to admins
// only.
func (a *clusterPoliciesBaseClient) GetByPolicyId(ctx context.Context, policyId string) (*Policy, error) {
	return a.clusterPoliciesImpl.Get(ctx, GetClusterPolicyRequest{
		PolicyId: policyId,
	})
}

// Get cluster policy permission levels.
//
// Gets the permission levels that a user can have on an object.
func (a *clusterPoliciesBaseClient) GetPermissionLevelsByClusterPolicyId(ctx context.Context, clusterPolicyId string) (*GetClusterPolicyPermissionLevelsResponse, error) {
	return a.clusterPoliciesImpl.GetPermissionLevels(ctx, GetClusterPolicyPermissionLevelsRequest{
		ClusterPolicyId: clusterPolicyId,
	})
}

// Get cluster policy permissions.
//
// Gets the permissions of a cluster policy. Cluster policies can inherit
// permissions from their root object.
func (a *clusterPoliciesBaseClient) GetPermissionsByClusterPolicyId(ctx context.Context, clusterPolicyId string) (*ClusterPolicyPermissions, error) {
	return a.clusterPoliciesImpl.GetPermissions(ctx, GetClusterPolicyPermissionsRequest{
		ClusterPolicyId: clusterPolicyId,
	})
}

// PolicyNameToPolicyIdMap calls [clusterPoliciesBaseClient.ListAll] and creates a map of results with [Policy].Name as key and [Policy].PolicyId as value.
//
// Returns an error if there's more than one [Policy] with the same .Name.
//
// Note: All [Policy] instances are loaded into memory before creating a map.
//
// This method is generated by Databricks SDK Code Generator.
func (a *clusterPoliciesBaseClient) PolicyNameToPolicyIdMap(ctx context.Context, request ListClusterPoliciesRequest) (map[string]string, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "name-to-id")
	mapping := map[string]string{}
	result, err := a.ListAll(ctx, request)
	if err != nil {
		return nil, err
	}
	for _, v := range result {
		key := v.Name
		_, duplicate := mapping[key]
		if duplicate {
			return nil, fmt.Errorf("duplicate .Name: %s", key)
		}
		mapping[key] = v.PolicyId
	}
	return mapping, nil
}

// GetByName calls [clusterPoliciesBaseClient.PolicyNameToPolicyIdMap] and returns a single [Policy].
//
// Returns an error if there's more than one [Policy] with the same .Name.
//
// Note: All [Policy] instances are loaded into memory before returning matching by name.
//
// This method is generated by Databricks SDK Code Generator.
func (a *clusterPoliciesBaseClient) GetByName(ctx context.Context, name string) (*Policy, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "get-by-name")
	result, err := a.ListAll(ctx, ListClusterPoliciesRequest{})
	if err != nil {
		return nil, err
	}
	tmp := map[string][]Policy{}
	for _, v := range result {
		key := v.Name
		tmp[key] = append(tmp[key], v)
	}
	alternatives, ok := tmp[name]
	if !ok || len(alternatives) == 0 {
		return nil, fmt.Errorf("Policy named '%s' does not exist", name)
	}
	if len(alternatives) > 1 {
		return nil, fmt.Errorf("there are %d instances of Policy named '%s'", len(alternatives), name)
	}
	return &alternatives[0], nil
}

type clustersBaseClient struct {
	clustersImpl
}

// Create new cluster.
//
// Creates a new Spark cluster. This method will acquire new instances from the
// cloud provider if necessary. This method is asynchronous; the returned
// “cluster_id“ can be used to poll the cluster status. When this method
// returns, the cluster will be in a “PENDING“ state. The cluster will be
// usable once it enters a “RUNNING“ state. Note: Databricks may not be able
// to acquire some of the requested nodes, due to cloud provider limitations
// (account limits, spot price, etc.) or transient network issues.
//
// If Databricks acquires at least 85% of the requested on-demand nodes, cluster
// creation will succeed. Otherwise the cluster will terminate with an
// informative error message.
//
// Rather than authoring the cluster's JSON definition from scratch, Databricks
// recommends filling out the [create compute UI] and then copying the generated
// JSON definition from the UI.
//
// [create compute UI]: https://docs.databricks.com/compute/configure.html
func (a *clustersBaseClient) Create(ctx context.Context, createCluster CreateCluster) (*ClustersCreateWaiter, error) {
	createClusterResponse, err := a.clustersImpl.Create(ctx, createCluster)
	if err != nil {
		return nil, err
	}
	return &ClustersCreateWaiter{
		RawResponse: createClusterResponse,
		clusterId:   createClusterResponse.ClusterId,
		service:     a,
	}, nil
}

type ClustersCreateWaiter struct {
	// RawResponse is the raw response of the Create call.
	RawResponse *CreateClusterResponse
	service     *clustersBaseClient
	clusterId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *ClustersCreateWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*ClusterDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[ClusterDetails](ctx, opts.Timeout, func() (*ClusterDetails, *retries.Err) {
		clusterDetails, err := w.service.Get(ctx, GetClusterRequest{
			ClusterId: w.clusterId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := clusterDetails.State
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case StateRunning: // target state
			return clusterDetails, nil
		case StateError, StateTerminated:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				StateRunning, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

// Terminate cluster.
//
// Terminates the Spark cluster with the specified ID. The cluster is removed
// asynchronously. Once the termination has completed, the cluster will be in a
// `TERMINATED` state. If the cluster is already in a `TERMINATING` or
// `TERMINATED` state, nothing will happen.
func (a *clustersBaseClient) Delete(ctx context.Context, deleteCluster DeleteCluster) (*ClustersDeleteWaiter, error) {
	deleteClusterResponse, err := a.clustersImpl.Delete(ctx, deleteCluster)
	if err != nil {
		return nil, err
	}
	return &ClustersDeleteWaiter{
		RawResponse: deleteClusterResponse,
		clusterId:   deleteCluster.ClusterId,
		service:     a,
	}, nil
}

type ClustersDeleteWaiter struct {
	// RawResponse is the raw response of the Delete call.
	RawResponse *DeleteClusterResponse
	service     *clustersBaseClient
	clusterId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *ClustersDeleteWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*ClusterDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[ClusterDetails](ctx, opts.Timeout, func() (*ClusterDetails, *retries.Err) {
		clusterDetails, err := w.service.Get(ctx, GetClusterRequest{
			ClusterId: w.clusterId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := clusterDetails.State
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case StateTerminated: // target state
			return clusterDetails, nil
		case StateError:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				StateTerminated, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

// Terminate cluster.
//
// Terminates the Spark cluster with the specified ID. The cluster is removed
// asynchronously. Once the termination has completed, the cluster will be in a
// `TERMINATED` state. If the cluster is already in a `TERMINATING` or
// `TERMINATED` state, nothing will happen.
func (a *clustersBaseClient) DeleteByClusterId(ctx context.Context, clusterId string) (*DeleteClusterResponse, error) {
	return a.clustersImpl.Delete(ctx, DeleteCluster{
		ClusterId: clusterId,
	})
}

// Update cluster configuration.
//
// Updates the configuration of a cluster to match the provided attributes and
// size. A cluster can be updated if it is in a `RUNNING` or `TERMINATED` state.
//
// If a cluster is updated while in a `RUNNING` state, it will be restarted so
// that the new attributes can take effect.
//
// If a cluster is updated while in a `TERMINATED` state, it will remain
// `TERMINATED`. The next time it is started using the `clusters/start` API, the
// new attributes will take effect. Any attempt to update a cluster in any other
// state will be rejected with an `INVALID_STATE` error code.
//
// Clusters created by the Databricks Jobs service cannot be edited.
func (a *clustersBaseClient) Edit(ctx context.Context, editCluster EditCluster) (*ClustersEditWaiter, error) {
	editClusterResponse, err := a.clustersImpl.Edit(ctx, editCluster)
	if err != nil {
		return nil, err
	}
	return &ClustersEditWaiter{
		RawResponse: editClusterResponse,
		clusterId:   editCluster.ClusterId,
		service:     a,
	}, nil
}

type ClustersEditWaiter struct {
	// RawResponse is the raw response of the Edit call.
	RawResponse *EditClusterResponse
	service     *clustersBaseClient
	clusterId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *ClustersEditWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*ClusterDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[ClusterDetails](ctx, opts.Timeout, func() (*ClusterDetails, *retries.Err) {
		clusterDetails, err := w.service.Get(ctx, GetClusterRequest{
			ClusterId: w.clusterId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := clusterDetails.State
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case StateRunning: // target state
			return clusterDetails, nil
		case StateError, StateTerminated:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				StateRunning, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

// Get cluster info.
//
// Retrieves the information for a cluster given its identifier. Clusters can be
// described while they are running, or up to 60 days after they are terminated.
func (a *clustersBaseClient) GetByClusterId(ctx context.Context, clusterId string) (*ClusterDetails, error) {
	return a.clustersImpl.Get(ctx, GetClusterRequest{
		ClusterId: clusterId,
	})
}

// Get cluster permission levels.
//
// Gets the permission levels that a user can have on an object.
func (a *clustersBaseClient) GetPermissionLevelsByClusterId(ctx context.Context, clusterId string) (*GetClusterPermissionLevelsResponse, error) {
	return a.clustersImpl.GetPermissionLevels(ctx, GetClusterPermissionLevelsRequest{
		ClusterId: clusterId,
	})
}

// Get cluster permissions.
//
// Gets the permissions of a cluster. Clusters can inherit permissions from
// their root object.
func (a *clustersBaseClient) GetPermissionsByClusterId(ctx context.Context, clusterId string) (*ClusterPermissions, error) {
	return a.clustersImpl.GetPermissions(ctx, GetClusterPermissionsRequest{
		ClusterId: clusterId,
	})
}

// ClusterDetailsClusterNameToClusterIdMap calls [clustersBaseClient.ListAll] and creates a map of results with [ClusterDetails].ClusterName as key and [ClusterDetails].ClusterId as value.
//
// Returns an error if there's more than one [ClusterDetails] with the same .ClusterName.
//
// Note: All [ClusterDetails] instances are loaded into memory before creating a map.
//
// This method is generated by Databricks SDK Code Generator.
func (a *clustersBaseClient) ClusterDetailsClusterNameToClusterIdMap(ctx context.Context, request ListClustersRequest) (map[string]string, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "name-to-id")
	mapping := map[string]string{}
	result, err := a.ListAll(ctx, request)
	if err != nil {
		return nil, err
	}
	for _, v := range result {
		key := v.ClusterName
		_, duplicate := mapping[key]
		if duplicate {
			return nil, fmt.Errorf("duplicate .ClusterName: %s", key)
		}
		mapping[key] = v.ClusterId
	}
	return mapping, nil
}

// GetByClusterName calls [clustersBaseClient.ClusterDetailsClusterNameToClusterIdMap] and returns a single [ClusterDetails].
//
// Returns an error if there's more than one [ClusterDetails] with the same .ClusterName.
//
// Note: All [ClusterDetails] instances are loaded into memory before returning matching by name.
//
// This method is generated by Databricks SDK Code Generator.
func (a *clustersBaseClient) GetByClusterName(ctx context.Context, name string) (*ClusterDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "get-by-name")
	result, err := a.ListAll(ctx, ListClustersRequest{})
	if err != nil {
		return nil, err
	}
	tmp := map[string][]ClusterDetails{}
	for _, v := range result {
		key := v.ClusterName
		tmp[key] = append(tmp[key], v)
	}
	alternatives, ok := tmp[name]
	if !ok || len(alternatives) == 0 {
		return nil, fmt.Errorf("ClusterDetails named '%s' does not exist", name)
	}
	if len(alternatives) > 1 {
		return nil, fmt.Errorf("there are %d instances of ClusterDetails named '%s'", len(alternatives), name)
	}
	return &alternatives[0], nil
}

// Permanently delete cluster.
//
// Permanently deletes a Spark cluster. This cluster is terminated and resources
// are asynchronously removed.
//
// In addition, users will no longer see permanently deleted clusters in the
// cluster list, and API users can no longer perform any action on permanently
// deleted clusters.
func (a *clustersBaseClient) PermanentDeleteByClusterId(ctx context.Context, clusterId string) (*PermanentDeleteClusterResponse, error) {
	return a.clustersImpl.PermanentDelete(ctx, PermanentDeleteCluster{
		ClusterId: clusterId,
	})
}

// Pin cluster.
//
// Pinning a cluster ensures that the cluster will always be returned by the
// ListClusters API. Pinning a cluster that is already pinned will have no
// effect. This API can only be called by workspace admins.
func (a *clustersBaseClient) PinByClusterId(ctx context.Context, clusterId string) (*PinClusterResponse, error) {
	return a.clustersImpl.Pin(ctx, PinCluster{
		ClusterId: clusterId,
	})
}

// Resize cluster.
//
// Resizes a cluster to have a desired number of workers. This will fail unless
// the cluster is in a `RUNNING` state.
func (a *clustersBaseClient) Resize(ctx context.Context, resizeCluster ResizeCluster) (*ClustersResizeWaiter, error) {
	resizeClusterResponse, err := a.clustersImpl.Resize(ctx, resizeCluster)
	if err != nil {
		return nil, err
	}
	return &ClustersResizeWaiter{
		RawResponse: resizeClusterResponse,
		clusterId:   resizeCluster.ClusterId,
		service:     a,
	}, nil
}

type ClustersResizeWaiter struct {
	// RawResponse is the raw response of the Resize call.
	RawResponse *ResizeClusterResponse
	service     *clustersBaseClient
	clusterId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *ClustersResizeWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*ClusterDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[ClusterDetails](ctx, opts.Timeout, func() (*ClusterDetails, *retries.Err) {
		clusterDetails, err := w.service.Get(ctx, GetClusterRequest{
			ClusterId: w.clusterId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := clusterDetails.State
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case StateRunning: // target state
			return clusterDetails, nil
		case StateError, StateTerminated:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				StateRunning, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

// Restart cluster.
//
// Restarts a Spark cluster with the supplied ID. If the cluster is not
// currently in a `RUNNING` state, nothing will happen.
func (a *clustersBaseClient) Restart(ctx context.Context, restartCluster RestartCluster) (*ClustersRestartWaiter, error) {
	restartClusterResponse, err := a.clustersImpl.Restart(ctx, restartCluster)
	if err != nil {
		return nil, err
	}
	return &ClustersRestartWaiter{
		RawResponse: restartClusterResponse,
		clusterId:   restartCluster.ClusterId,
		service:     a,
	}, nil
}

type ClustersRestartWaiter struct {
	// RawResponse is the raw response of the Restart call.
	RawResponse *RestartClusterResponse
	service     *clustersBaseClient
	clusterId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *ClustersRestartWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*ClusterDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[ClusterDetails](ctx, opts.Timeout, func() (*ClusterDetails, *retries.Err) {
		clusterDetails, err := w.service.Get(ctx, GetClusterRequest{
			ClusterId: w.clusterId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := clusterDetails.State
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case StateRunning: // target state
			return clusterDetails, nil
		case StateError, StateTerminated:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				StateRunning, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

// Start terminated cluster.
//
// Starts a terminated Spark cluster with the supplied ID. This works similar to
// `createCluster` except: - The previous cluster id and attributes are
// preserved. - The cluster starts with the last specified cluster size. - If
// the previous cluster was an autoscaling cluster, the current cluster starts
// with the minimum number of nodes. - If the cluster is not currently in a
// “TERMINATED“ state, nothing will happen. - Clusters launched to run a job
// cannot be started.
func (a *clustersBaseClient) Start(ctx context.Context, startCluster StartCluster) (*ClustersStartWaiter, error) {
	startClusterResponse, err := a.clustersImpl.Start(ctx, startCluster)
	if err != nil {
		return nil, err
	}
	return &ClustersStartWaiter{
		RawResponse: startClusterResponse,
		clusterId:   startCluster.ClusterId,
		service:     a,
	}, nil
}

type ClustersStartWaiter struct {
	// RawResponse is the raw response of the Start call.
	RawResponse *StartClusterResponse
	service     *clustersBaseClient
	clusterId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *ClustersStartWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*ClusterDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[ClusterDetails](ctx, opts.Timeout, func() (*ClusterDetails, *retries.Err) {
		clusterDetails, err := w.service.Get(ctx, GetClusterRequest{
			ClusterId: w.clusterId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := clusterDetails.State
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case StateRunning: // target state
			return clusterDetails, nil
		case StateError, StateTerminated:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				StateRunning, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

// Start terminated cluster.
//
// Starts a terminated Spark cluster with the supplied ID. This works similar to
// `createCluster` except: - The previous cluster id and attributes are
// preserved. - The cluster starts with the last specified cluster size. - If
// the previous cluster was an autoscaling cluster, the current cluster starts
// with the minimum number of nodes. - If the cluster is not currently in a
// “TERMINATED“ state, nothing will happen. - Clusters launched to run a job
// cannot be started.
func (a *clustersBaseClient) StartByClusterId(ctx context.Context, clusterId string) (*StartClusterResponse, error) {
	return a.clustersImpl.Start(ctx, StartCluster{
		ClusterId: clusterId,
	})
}

// Unpin cluster.
//
// Unpinning a cluster will allow the cluster to eventually be removed from the
// ListClusters API. Unpinning a cluster that is not pinned will have no effect.
// This API can only be called by workspace admins.
func (a *clustersBaseClient) UnpinByClusterId(ctx context.Context, clusterId string) (*UnpinClusterResponse, error) {
	return a.clustersImpl.Unpin(ctx, UnpinCluster{
		ClusterId: clusterId,
	})
}

// Update cluster configuration (partial).
//
// Updates the configuration of a cluster to match the partial set of attributes
// and size. Denote which fields to update using the `update_mask` field in the
// request body. A cluster can be updated if it is in a `RUNNING` or
// `TERMINATED` state. If a cluster is updated while in a `RUNNING` state, it
// will be restarted so that the new attributes can take effect. If a cluster is
// updated while in a `TERMINATED` state, it will remain `TERMINATED`. The
// updated attributes will take effect the next time the cluster is started
// using the `clusters/start` API. Attempts to update a cluster in any other
// state will be rejected with an `INVALID_STATE` error code. Clusters created
// by the Databricks Jobs service cannot be updated.
func (a *clustersBaseClient) Update(ctx context.Context, updateCluster UpdateCluster) (*ClustersUpdateWaiter, error) {
	updateClusterResponse, err := a.clustersImpl.Update(ctx, updateCluster)
	if err != nil {
		return nil, err
	}
	return &ClustersUpdateWaiter{
		RawResponse: updateClusterResponse,
		clusterId:   updateCluster.ClusterId,
		service:     a,
	}, nil
}

type ClustersUpdateWaiter struct {
	// RawResponse is the raw response of the Update call.
	RawResponse *UpdateClusterResponse
	service     *clustersBaseClient
	clusterId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *ClustersUpdateWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*ClusterDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[ClusterDetails](ctx, opts.Timeout, func() (*ClusterDetails, *retries.Err) {
		clusterDetails, err := w.service.Get(ctx, GetClusterRequest{
			ClusterId: w.clusterId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := clusterDetails.State
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case StateRunning: // target state
			return clusterDetails, nil
		case StateError, StateTerminated:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				StateRunning, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

type commandExecutionBaseClient struct {
	commandExecutionImpl
}

// Cancel a command.
//
// Cancels a currently running command within an execution context.
//
// The command ID is obtained from a prior successful call to __execute__.
func (a *commandExecutionBaseClient) Cancel(ctx context.Context, cancelCommand CancelCommand) (*CommandExecutionCancelWaiter, error) {
	cancelResponse, err := a.commandExecutionImpl.Cancel(ctx, cancelCommand)
	if err != nil {
		return nil, err
	}
	return &CommandExecutionCancelWaiter{
		RawResponse: cancelResponse,
		clusterId:   cancelCommand.ClusterId,
		commandId:   cancelCommand.CommandId,
		contextId:   cancelCommand.ContextId,
		service:     a,
	}, nil
}

type CommandExecutionCancelWaiter struct {
	// RawResponse is the raw response of the Cancel call.
	RawResponse *CancelResponse
	service     *commandExecutionBaseClient
	clusterId   string
	commandId   string
	contextId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *CommandExecutionCancelWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*CommandStatusResponse, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[CommandStatusResponse](ctx, opts.Timeout, func() (*CommandStatusResponse, *retries.Err) {
		commandStatusResponse, err := w.service.CommandStatus(ctx, CommandStatusRequest{
			ClusterId: w.clusterId,
			CommandId: w.commandId,
			ContextId: w.contextId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := commandStatusResponse.Status
		statusMessage := fmt.Sprintf("current status: %s", status)
		if commandStatusResponse.Results != nil {
			statusMessage = commandStatusResponse.Results.Cause
		}
		switch status {
		case CommandStatusCancelled: // target state
			return commandStatusResponse, nil
		case CommandStatusError:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				CommandStatusCancelled, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

// Create an execution context.
//
// Creates an execution context for running cluster commands.
//
// If successful, this method returns the ID of the new execution context.
func (a *commandExecutionBaseClient) Create(ctx context.Context, createContext CreateContext) (*CommandExecutionCreateWaiter, error) {
	created, err := a.commandExecutionImpl.Create(ctx, createContext)
	if err != nil {
		return nil, err
	}
	return &CommandExecutionCreateWaiter{
		RawResponse: created,
		clusterId:   createContext.ClusterId,
		contextId:   created.Id,
		service:     a,
	}, nil
}

type CommandExecutionCreateWaiter struct {
	// RawResponse is the raw response of the Create call.
	RawResponse *Created
	service     *commandExecutionBaseClient
	clusterId   string
	contextId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *CommandExecutionCreateWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*ContextStatusResponse, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[ContextStatusResponse](ctx, opts.Timeout, func() (*ContextStatusResponse, *retries.Err) {
		contextStatusResponse, err := w.service.ContextStatus(ctx, ContextStatusRequest{
			ClusterId: w.clusterId,
			ContextId: w.contextId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := contextStatusResponse.Status
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case ContextStatusRunning: // target state
			return contextStatusResponse, nil
		case ContextStatusError:
			err := fmt.Errorf("failed to reach %s, got %s: %s",
				ContextStatusRunning, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

// Run a command.
//
// Runs a cluster command in the given execution context, using the provided
// language.
//
// If successful, it returns an ID for tracking the status of the command's
// execution.
func (a *commandExecutionBaseClient) Execute(ctx context.Context, command Command) (*CommandExecutionExecuteWaiter, error) {
	created, err := a.commandExecutionImpl.Execute(ctx, command)
	if err != nil {
		return nil, err
	}
	return &CommandExecutionExecuteWaiter{
		RawResponse: created,
		clusterId:   command.ClusterId,
		commandId:   created.Id,
		contextId:   command.ContextId,
		service:     a,
	}, nil
}

type CommandExecutionExecuteWaiter struct {
	// RawResponse is the raw response of the Execute call.
	RawResponse *Created
	service     *commandExecutionBaseClient
	clusterId   string
	commandId   string
	contextId   string
}

// Polls the server until the operation reaches a terminal state, encounters an error, or reaches a timeout defaults to 20 min.
// This method will return an error if a failure state is reached.
func (w *CommandExecutionExecuteWaiter) WaitUntilDone(ctx context.Context, opts *retries.WaitUntilDoneOptions) (*CommandStatusResponse, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "long-running")
	if opts == nil {
		opts = &retries.WaitUntilDoneOptions{}
	}
	if opts.Timeout == 0 {
		opts.Timeout = 20 * time.Minute
	}

	return retries.Poll[CommandStatusResponse](ctx, opts.Timeout, func() (*CommandStatusResponse, *retries.Err) {
		commandStatusResponse, err := w.service.CommandStatus(ctx, CommandStatusRequest{
			ClusterId: w.clusterId,
			CommandId: w.commandId,
			ContextId: w.contextId,
		})
		if err != nil {
			return nil, retries.Halt(err)
		}
		status := commandStatusResponse.Status
		statusMessage := fmt.Sprintf("current status: %s", status)
		switch status {
		case CommandStatusFinished, CommandStatusError: // target state
			return commandStatusResponse, nil
		case CommandStatusCancelled, CommandStatusCancelling:
			err := fmt.Errorf("failed to reach %s or %s, got %s: %s",
				CommandStatusFinished, CommandStatusError, status, statusMessage)
			return nil, retries.Halt(err)
		default:
			return nil, retries.Continues(statusMessage)
		}
	})

}

type globalInitScriptsBaseClient struct {
	globalInitScriptsImpl
}

// Delete init script.
//
// Deletes a global init script.
func (a *globalInitScriptsBaseClient) DeleteByScriptId(ctx context.Context, scriptId string) (*DeleteResponse, error) {
	return a.globalInitScriptsImpl.Delete(ctx, DeleteGlobalInitScriptRequest{
		ScriptId: scriptId,
	})
}

// Get an init script.
//
// Gets all the details of a script, including its Base64-encoded contents.
func (a *globalInitScriptsBaseClient) GetByScriptId(ctx context.Context, scriptId string) (*GlobalInitScriptDetailsWithContent, error) {
	return a.globalInitScriptsImpl.Get(ctx, GetGlobalInitScriptRequest{
		ScriptId: scriptId,
	})
}

// GlobalInitScriptDetailsNameToScriptIdMap calls [globalInitScriptsBaseClient.ListAll] and creates a map of results with [GlobalInitScriptDetails].Name as key and [GlobalInitScriptDetails].ScriptId as value.
//
// Returns an error if there's more than one [GlobalInitScriptDetails] with the same .Name.
//
// Note: All [GlobalInitScriptDetails] instances are loaded into memory before creating a map.
//
// This method is generated by Databricks SDK Code Generator.
func (a *globalInitScriptsBaseClient) GlobalInitScriptDetailsNameToScriptIdMap(ctx context.Context) (map[string]string, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "name-to-id")
	mapping := map[string]string{}
	result, err := a.ListAll(ctx)
	if err != nil {
		return nil, err
	}
	for _, v := range result {
		key := v.Name
		_, duplicate := mapping[key]
		if duplicate {
			return nil, fmt.Errorf("duplicate .Name: %s", key)
		}
		mapping[key] = v.ScriptId
	}
	return mapping, nil
}

// GetByName calls [globalInitScriptsBaseClient.GlobalInitScriptDetailsNameToScriptIdMap] and returns a single [GlobalInitScriptDetails].
//
// Returns an error if there's more than one [GlobalInitScriptDetails] with the same .Name.
//
// Note: All [GlobalInitScriptDetails] instances are loaded into memory before returning matching by name.
//
// This method is generated by Databricks SDK Code Generator.
func (a *globalInitScriptsBaseClient) GetByName(ctx context.Context, name string) (*GlobalInitScriptDetails, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "get-by-name")
	result, err := a.ListAll(ctx)
	if err != nil {
		return nil, err
	}
	tmp := map[string][]GlobalInitScriptDetails{}
	for _, v := range result {
		key := v.Name
		tmp[key] = append(tmp[key], v)
	}
	alternatives, ok := tmp[name]
	if !ok || len(alternatives) == 0 {
		return nil, fmt.Errorf("GlobalInitScriptDetails named '%s' does not exist", name)
	}
	if len(alternatives) > 1 {
		return nil, fmt.Errorf("there are %d instances of GlobalInitScriptDetails named '%s'", len(alternatives), name)
	}
	return &alternatives[0], nil
}

type instancePoolsBaseClient struct {
	instancePoolsImpl
}

// Delete an instance pool.
//
// Deletes the instance pool permanently. The idle instances in the pool are
// terminated asynchronously.
func (a *instancePoolsBaseClient) DeleteByInstancePoolId(ctx context.Context, instancePoolId string) (*DeleteInstancePoolResponse, error) {
	return a.instancePoolsImpl.Delete(ctx, DeleteInstancePool{
		InstancePoolId: instancePoolId,
	})
}

// Get instance pool information.
//
// Retrieve the information for an instance pool based on its identifier.
func (a *instancePoolsBaseClient) GetByInstancePoolId(ctx context.Context, instancePoolId string) (*GetInstancePool, error) {
	return a.instancePoolsImpl.Get(ctx, GetInstancePoolRequest{
		InstancePoolId: instancePoolId,
	})
}

// Get instance pool permission levels.
//
// Gets the permission levels that a user can have on an object.
func (a *instancePoolsBaseClient) GetPermissionLevelsByInstancePoolId(ctx context.Context, instancePoolId string) (*GetInstancePoolPermissionLevelsResponse, error) {
	return a.instancePoolsImpl.GetPermissionLevels(ctx, GetInstancePoolPermissionLevelsRequest{
		InstancePoolId: instancePoolId,
	})
}

// Get instance pool permissions.
//
// Gets the permissions of an instance pool. Instance pools can inherit
// permissions from their root object.
func (a *instancePoolsBaseClient) GetPermissionsByInstancePoolId(ctx context.Context, instancePoolId string) (*InstancePoolPermissions, error) {
	return a.instancePoolsImpl.GetPermissions(ctx, GetInstancePoolPermissionsRequest{
		InstancePoolId: instancePoolId,
	})
}

// InstancePoolAndStatsInstancePoolNameToInstancePoolIdMap calls [instancePoolsBaseClient.ListAll] and creates a map of results with [InstancePoolAndStats].InstancePoolName as key and [InstancePoolAndStats].InstancePoolId as value.
//
// Returns an error if there's more than one [InstancePoolAndStats] with the same .InstancePoolName.
//
// Note: All [InstancePoolAndStats] instances are loaded into memory before creating a map.
//
// This method is generated by Databricks SDK Code Generator.
func (a *instancePoolsBaseClient) InstancePoolAndStatsInstancePoolNameToInstancePoolIdMap(ctx context.Context) (map[string]string, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "name-to-id")
	mapping := map[string]string{}
	result, err := a.ListAll(ctx)
	if err != nil {
		return nil, err
	}
	for _, v := range result {
		key := v.InstancePoolName
		_, duplicate := mapping[key]
		if duplicate {
			return nil, fmt.Errorf("duplicate .InstancePoolName: %s", key)
		}
		mapping[key] = v.InstancePoolId
	}
	return mapping, nil
}

// GetByInstancePoolName calls [instancePoolsBaseClient.InstancePoolAndStatsInstancePoolNameToInstancePoolIdMap] and returns a single [InstancePoolAndStats].
//
// Returns an error if there's more than one [InstancePoolAndStats] with the same .InstancePoolName.
//
// Note: All [InstancePoolAndStats] instances are loaded into memory before returning matching by name.
//
// This method is generated by Databricks SDK Code Generator.
func (a *instancePoolsBaseClient) GetByInstancePoolName(ctx context.Context, name string) (*InstancePoolAndStats, error) {
	ctx = useragent.InContext(ctx, "sdk-feature", "get-by-name")
	result, err := a.ListAll(ctx)
	if err != nil {
		return nil, err
	}
	tmp := map[string][]InstancePoolAndStats{}
	for _, v := range result {
		key := v.InstancePoolName
		tmp[key] = append(tmp[key], v)
	}
	alternatives, ok := tmp[name]
	if !ok || len(alternatives) == 0 {
		return nil, fmt.Errorf("InstancePoolAndStats named '%s' does not exist", name)
	}
	if len(alternatives) > 1 {
		return nil, fmt.Errorf("there are %d instances of InstancePoolAndStats named '%s'", len(alternatives), name)
	}
	return &alternatives[0], nil
}

type instanceProfilesBaseClient struct {
	instanceProfilesImpl
}

// Remove the instance profile.
//
// Remove the instance profile with the provided ARN. Existing clusters with
// this instance profile will continue to function.
//
// This API is only accessible to admin users.
func (a *instanceProfilesBaseClient) RemoveByInstanceProfileArn(ctx context.Context, instanceProfileArn string) (*RemoveResponse, error) {
	return a.instanceProfilesImpl.Remove(ctx, RemoveInstanceProfile{
		InstanceProfileArn: instanceProfileArn,
	})
}

type librariesBaseClient struct {
	librariesImpl
}

// Get status.
//
// Get the status of libraries on a cluster. A status is returned for all
// libraries installed on this cluster via the API or the libraries UI. The
// order of returned libraries is as follows: 1. Libraries set to be installed
// on this cluster, in the order that the libraries were added to the cluster,
// are returned first. 2. Libraries that were previously requested to be
// installed on this cluster or, but are now marked for removal, in no
// particular order, are returned last.
func (a *librariesBaseClient) ClusterStatusByClusterId(ctx context.Context, clusterId string) (*ClusterLibraryStatuses, error) {
	return a.librariesImpl.internalClusterStatus(ctx, ClusterStatus{
		ClusterId: clusterId,
	})
}

type policyComplianceForClustersBaseClient struct {
	policyComplianceForClustersImpl
}

// Get cluster policy compliance.
//
// Returns the policy compliance status of a cluster. Clusters could be out of
// compliance if their policy was updated after the cluster was last edited.
func (a *policyComplianceForClustersBaseClient) GetComplianceByClusterId(ctx context.Context, clusterId string) (*GetClusterComplianceResponse, error) {
	return a.policyComplianceForClustersImpl.GetCompliance(ctx, GetClusterComplianceRequest{
		ClusterId: clusterId,
	})
}

type policyFamiliesBaseClient struct {
	policyFamiliesImpl
}

// Get policy family information.
//
// Retrieve the information for an policy family based on its identifier and
// version
func (a *policyFamiliesBaseClient) GetByPolicyFamilyId(ctx context.Context, policyFamilyId string) (*PolicyFamily, error) {
	return a.policyFamiliesImpl.Get(ctx, GetPolicyFamilyRequest{
		PolicyFamilyId: policyFamilyId,
	})
}
